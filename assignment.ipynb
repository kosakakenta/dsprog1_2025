{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa17c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "START_URL = \"https://www.musashino-u.ac.jp/\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TIMEOUT = 7\n",
    "\n",
    "def host(url):\n",
    "    for p in (\"http://\", \"https://\"):\n",
    "        if url.startswith(p):\n",
    "            return url[len(p):].split(\"/\", 1)[0]\n",
    "    return url.split(\"/\", 1)[0]\n",
    "\n",
    "ALLOW_HOST = host(START_URL)\n",
    "\n",
    "def normalize(url):\n",
    "    return url.split(\"#\", 1)[0].rstrip(\"/\")\n",
    "\n",
    "def to_abs(base, href):\n",
    "    if not href:\n",
    "        return None\n",
    "    h = href.strip()\n",
    "    l = h.lower()\n",
    "    if l.startswith((\"javascript:\", \"mailto:\", \"tel:\")):\n",
    "        return None\n",
    "    if l.startswith((\"http://\", \"https://\")):\n",
    "        return normalize(h)\n",
    "\n",
    "    scheme = \"https://\" if base.startswith(\"https://\") else \"http://\"\n",
    "    b = base[len(scheme):]\n",
    "    hostpart, _, basepath = b.partition(\"/\")\n",
    "    basepath = \"/\" + basepath if basepath else \"/\"\n",
    "\n",
    "    if h.startswith(\"/\"):\n",
    "        return normalize(f\"{scheme}{hostpart}{h}\")\n",
    "\n",
    "    base_dirs = [p for p in basepath.split(\"/\") if p]\n",
    "    if not basepath.endswith(\"/\") and base_dirs:\n",
    "        base_dirs.pop()\n",
    "\n",
    "    segs = [p for p in h.split(\"/\") if p]\n",
    "    res = []\n",
    "    for s in segs:\n",
    "        if s == \"..\":\n",
    "            if base_dirs:\n",
    "                base_dirs.pop()\n",
    "        elif s != \".\":\n",
    "            res.append(s)\n",
    "\n",
    "    path = \"/\".join(base_dirs + res)\n",
    "    return normalize(f\"{scheme}{hostpart}/\" + path)\n",
    "\n",
    "def same_host(url):\n",
    "    return host(url) == ALLOW_HOST\n",
    "\n",
    "def fetch(url):\n",
    "    try:\n",
    "        r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "    if r.status_code != 200:\n",
    "        return None\n",
    "    ct = r.headers.get(\"Content-Type\", \"\").lower()\n",
    "    if (\"html\" not in ct) and (\"text/\" not in ct):\n",
    "        return None\n",
    "    enc = r.apparent_encoding\n",
    "    if \"charset=\" in ct:\n",
    "        enc = ct.split(\"charset=\")[-1].split(\";\")[0].strip() or enc\n",
    "    r.encoding = enc or \"utf-8\"\n",
    "    return r.text\n",
    "\n",
    "def title_of(html):\n",
    "    if not html:\n",
    "        return \"\"\n",
    "    s = BeautifulSoup(html, \"html.parser\")\n",
    "    t = s.find(\"title\")\n",
    "    return (t.get_text() if t else \"\").strip()\n",
    "\n",
    "def links(html, base):\n",
    "    if not html:\n",
    "        return []\n",
    "    s = BeautifulSoup(html, \"html.parser\")\n",
    "    for c in s.find_all(string=lambda x: isinstance(x, Comment)):\n",
    "        c.extract()\n",
    "    out = []\n",
    "    for a in s.find_all(\"a\", href=True):\n",
    "        u = to_abs(base, a[\"href\"])\n",
    "        if u and same_host(u):\n",
    "            out.append(u)\n",
    "    return out\n",
    "\n",
    "def crawl(start):\n",
    "    stack = [normalize(start)]\n",
    "    seen = set()\n",
    "    result = {}\n",
    "    while stack:\n",
    "        url = stack.pop()\n",
    "        if url in seen or not same_host(url):\n",
    "            continue\n",
    "        html = fetch(url)\n",
    "        seen.add(url)\n",
    "        result[url] = title_of(html)\n",
    "        time.sleep(1.0)\n",
    "        if html:\n",
    "            for v in links(html, url):\n",
    "                v = normalize(v)\n",
    "                if v not in seen:\n",
    "                    stack.append(v)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = crawl(START_URL)\n",
    "    print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
